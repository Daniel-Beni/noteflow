import pyaudio
import pkg_resources
import webrtcvad
import queue
from collections import deque
import numpy as np

# ==============================
# ParamÃ¨tres gÃ©nÃ©raux
# ==============================
RATE = 16000                  # frÃ©quence d'Ã©chantillonnage (16 kHz = requis par Whisper)
CHANNELS = 1                  # mono
FORMAT = pyaudio.paInt16      # PCM 16 bits
FRAME_MS = 30                 # taille des frames analysÃ©es par VAD (10, 20 ou 30 ms)
FRAME_BYTES = int(RATE * (FRAME_MS / 1000.0)) * 2  # nombre dâ€™octets par frame (int16 â†’ 2 octets)

# ==============================
# ParamÃ¨tres VAD (Voice Activity Detection)
# ==============================
AGGRESSIVENESS = 1            # 0 = tolÃ©rant, 3 = trÃ¨s strict (on coupe vite le bruit)
RING_MS = 500                 # fenÃªtre de dÃ©cision (ms)
RING_FRAMES = RING_MS // FRAME_MS

START_SPEECH_RATIO = 0.6      # proportion de frames "voix" dans la fenÃªtre â†’ dÃ©clenche un segment
STOP_SILENCE_RATIO = 0.95     # proportion de frames "non-voix" dans la fenÃªtre â†’ termine un segment

# Taille minimale dâ€™un segment valide avant de lâ€™envoyer Ã  Whisper
MIN_SEGMENT_SIZE = 4000       # ~0.25s dâ€™audio Ã  16kHz (16k * 0.25 * 2 bytes â‰ˆ 8000, ici en bytes bruts)


# ==============================
# Capture micro (PyAudio)
# ==============================
class MicStream:
    """
    Capture des frames audio depuis le micro en temps rÃ©el.
    Utilise PyAudio en mode callback pour remplir une queue.
    """
    def __init__(self, rate=RATE, channels=CHANNELS, frames_per_buffer=int(RATE * FRAME_MS / 1000.0)):
        self.pa = pyaudio.PyAudio()
        self.rate = rate
        self.channels = channels
        self.frames_per_buffer = frames_per_buffer
        self.stream = None
        self.q = queue.Queue()

    def start(self, device_index=None):
        """DÃ©marre le flux micro en callback â†’ remplit la queue"""
        def _callback(in_data, frame_count, time_info, status):
            self.q.put(in_data)
            return (None, pyaudio.paContinue)

        self.stream = self.pa.open(format=FORMAT,
                                channels=self.channels,
                                rate=self.rate,
                                input=True,
                                input_device_index=device_index,
                                frames_per_buffer=self.frames_per_buffer,
                                stream_callback=_callback)
        self.stream.start_stream()
        return self

    def frames(self):
        """ItÃ©rateur qui fournit des frames audio brutes depuis la queue"""
        while True:
            try:
                yield self.q.get(timeout=0.1)
            except queue.Empty:
                continue

    def stop(self):
        """ArrÃªte proprement le micro"""
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        self.pa.terminate()


# ==============================
# DÃ©tection de segments de parole (VAD)
# ==============================
class VADSegmenter:
    """
    DÃ©tecte la parole dans le flux audio en utilisant WebRTC VAD.
    Retourne des segments audio complets (paroles entre deux silences).
    """
    def __init__(self, rate=RATE, aggressiveness=AGGRESSIVENESS):
        self.vad = webrtcvad.Vad(aggressiveness)
        self.rate = rate
        self.ring = deque(maxlen=RING_FRAMES)  # tampon circulaire pour dÃ©cider start/stop
        self.triggered = False                 # Ã©tat : en segment ou pas
        self.voiced_buffer = []                # stockage temporaire des frames en cours

    def is_speech(self, frame_bytes):
        """Retourne True si la frame contient de la parole, False sinon"""
        return self.vad.is_speech(frame_bytes, self.rate)

    def process(self, frames_iter):
        """
        GÃ©nÃ©rateur qui yield des chunks audio (bytes PCM16) correspondant Ã  des segments parlÃ©s.
        """
        for frame in frames_iter:
            speech = self.is_speech(frame)
            self.ring.append((frame, speech))

            if not self.triggered:
                # ðŸ”¹ Phase attente : dÃ©clencher si beaucoup de frames "voix"
                if len(self.ring) == self.ring.maxlen:
                    voiced_count = sum(1 for _, s in self.ring if s)
                    if voiced_count >= START_SPEECH_RATIO * len(self.ring):
                        self.triggered = True
                        # inclure prÃ©-roll (dÃ©but stockÃ© dans ring)
                        self.voiced_buffer.extend(f for f, _ in self.ring)
                        self.ring.clear()
            else:
                # ðŸ”¹ Phase active : on accumule les frames
                self.voiced_buffer.append(frame)

                if len(self.ring) == self.ring.maxlen:
                    non_voiced = sum(1 for _, s in self.ring if not s)
                    if non_voiced >= STOP_SILENCE_RATIO * len(self.ring):
                        # fin du segment dÃ©tectÃ©e
                        chunk = b"".join(self.voiced_buffer)

                        if len(chunk) >= MIN_SEGMENT_SIZE:
                            yield chunk
                        else:
                            print(f"[DEBUG] Segment ignorÃ©, trop court ({len(chunk)} bytes)")

                        # reset Ã©tat
                        self.voiced_buffer.clear()
                        self.ring.clear()
                        self.triggered = False

        # Flush si on termine pendant qu'on est dÃ©clenchÃ©
        if self.voiced_buffer:
            chunk = b"".join(self.voiced_buffer)
            if len(chunk) >= MIN_SEGMENT_SIZE:
                yield chunk
            else:
                print(f"[DEBUG] Segment ignorÃ©, trop court ({len(chunk)} bytes)")
            self.voiced_buffer.clear()


# ==============================
# Utilitaires
# ==============================
def pcm16_bytes_to_float32(pcm_bytes):
    """
    Convertit bytes PCM16 mono en numpy float32 [-1, 1]
    â†’ format attendu par faster-whisper
    """
    return np.frombuffer(pcm_bytes, dtype=np.int16).astype(np.float32) / 32768.0
